{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notebook for the development of the model to power the face detection","metadata":{"id":"iSc4YZh28MKK"}},{"cell_type":"markdown","source":"### Imports","metadata":{"id":"EtDczes88MKM"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.patches as patches\nimport cv2\nimport torch\nfrom PIL import Image\nfrom torchvision.datasets import WIDERFace\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import ToTensor, Compose, Resize\nfrom torchvision.models.detection.faster_rcnn import fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, FastRCNNPredictor\nfrom collections import defaultdict\nimport os\nimport pathlib\nimport random\nfrom typing import Tuple, List","metadata":{"id":"OCUGwvvS8MKM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Constants","metadata":{"id":"nBMteezZayPi"}},{"cell_type":"code","source":"ROOT = \"/kaggle/working\" # Change to your root directory","metadata":{"id":"AqOoa7eaa04f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download Data","metadata":{"id":"S53zNofH8MKN"}},{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"WIDERFace(root=\"\", download=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ov52Toj8MKN","outputId":"1bc73a3b-56f2-4319-c5ea-b9211f32a523","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration","metadata":{"id":"Ztfv2OScbKhV"}},{"cell_type":"markdown","source":"### Build dictionary with image name and folder as key and a list with the number of faces and the coordinates of the bounding boxes as value","metadata":{"id":"s21-yTuLbNBT"}},{"cell_type":"code","source":"# Make list from ground truth .txt file\nground_truth_file_list = []\nwith open(ROOT + \"/widerface/wider_face_split/wider_face_train_bbx_gt.txt\") as file:\n  for line in file:\n    ground_truth_file_list.append(line.strip())","metadata":{"id":"EKHRuaNLhVzN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_ground_truth = {}\nindex = 0\nwhile index < len(ground_truth_file_list):\n  if len(ground_truth_file_list[index].split(\".\")) == 2:\n    image_name = ground_truth_file_list[index].strip()\n    index += 1\n    num_faces = int(ground_truth_file_list[index].strip())\n    ground_truth = [num_faces]\n    for _ in range(num_faces):\n      index += 1\n      ground_truth.append(list(map(lambda x: int(x), ground_truth_file_list[index].split(\" \")))[:4]) # only num_faces / x, y, width, height\n    images_ground_truth[image_name] = ground_truth\n  index += 1","metadata":{"id":"do3Fbl3Li8W4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize 4 random example images with bounding boxes","metadata":{"id":"OG9OQcpAminU"}},{"cell_type":"code","source":"def plot_image(axs, x, y, image):\n  ground_truth = images_ground_truth[image]\n  axs[x, y].imshow(mpimg.imread(ROOT + \"/widerface/WIDER_train/images/\" + image))\n  for rect in ground_truth[1:]:\n    rect_x, rect_y, width, height = rect\n    rect_obj = patches.Rectangle((rect_x, rect_y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n    axs[x, y].add_patch(rect_obj)\n  image_name = image.split(\"/\")[1].split(\".\")[0]\n  axs[x, y].set_title(image_name)","metadata":{"id":"rtZNkLfftILb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n\nimages = random.sample(list(images_ground_truth.keys()), 4)\n# Plot first image\nplot_image(axs, 0, 0, images[0])\n# Plot second image\nplot_image(axs, 0, 1, images[1])\n# Plot third image\nplot_image(axs, 1, 0, images[2])\n# Plot fourth image\nplot_image(axs, 1, 1, images[3])","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":618},"id":"fSQXeO54quCX","outputId":"1e34cd16-66e6-4389-e389-bee2d5748629","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explore Distribution over number of faces\n-> Use log scale to better visualize extreme values","metadata":{"id":"KBTsibc5arGm"}},{"cell_type":"code","source":"dist_num_faces = defaultdict(int)\nwith open(ROOT + \"/widerface/wider_face_split/wider_face_train_bbx_gt.txt\") as file:\n  for line in file:\n    if len(line.split(\" \")) == 1 and len(line.split(\".\")) == 1:\n      dist_num_faces[int(line)] += 1\nprint(f\"There are {dist_num_faces[0]} images with zero faces.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TJJqndfX_h5A","outputId":"3685c14b-4754-4381-afb7-2d5b40faa576","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_faces = list(dist_num_faces.keys())\nfrequency = list(dist_num_faces.values())\n\nplt.bar(num_faces, frequency, log=True)\nplt.xlabel('Number of faces in image')\nplt.ylabel('Number of images')\nplt.title('Distribution of images by number of faces')\n\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"RJfQ-TL_ZssA","outputId":"4f9205b5-da98-41c9-a53b-7a4430289cb5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Dataset class","metadata":{"id":"R-MlT0j1dc7f"}},{"cell_type":"code","source":"class FaceImageDataset(Dataset):\n  \"\"\"Dataset class for the WIDERFACE dataset.\n\n  Attributes:\n    data_dir (str): path to the directory with image directories in it.\n    transform: transformation to perform on images.\n  \"\"\"\n\n\n  def __init__(self, data_dir: str, ground_truth_file: str, transform=None) -> None:\n    \"\"\"Constructs Dataset class.\n\n    Attributes:\n      data_dir (str): path to the directory with image directories in it.\n      transform: transformation to perform on images.\n    \"\"\"\n    # Paths to the images\n    self.__paths = list(map(lambda path: str(path), pathlib.Path(data_dir).glob(\"*/*.jpg\")))\n    self.__ground_truth_file = ground_truth_file\n    self.__transform = transform\n    self.__label_data = self.__get_label_data()\n\n\n  def __get_label_data(self) -> dict:\n    \"\"\"Return dictionary with number of faces and bounding boxes for all images.\n    \"\"\"\n    label_file = self.__get_label_file()\n    label_data = {}\n    index = 0\n    index_image = 0\n    while index < len(label_file):\n      if len(label_file[index].split(\".\")) == 2:\n        image_name = label_file[index].strip()\n        index += 1\n        num_faces = int(label_file[index].strip())\n        boxes = []\n        image = self.__load_image(index_image)\n        index_image += 1\n        original_height, original_width = image.shape[1:]\n        for _ in range(num_faces):\n          index += 1\n          # only num_faces / x, y, width, height\n          box_data = list(map(lambda x: int(x), label_file[index].split(\" \")))[:4]\n          if box_data[2] > 0 and box_data[3] > 0:\n            x_0 = box_data[0] / original_width * 320\n            x_1 = (box_data[0] + box_data[2]) / original_width * 320\n            y_0 = box_data[1] / original_height * 320\n            y_1 = (box_data[1] + box_data[3]) / original_height * 320\n            box = [min(x_0, x_1), min(y_0, y_1), max(x_0, x_1), max(y_0, y_1)]\n            boxes.append(box)\n          else:\n            num_faces -= 1\n        label = {\"labels\": torch.tensor([1 for _ in range(num_faces)], dtype=torch.int64), \"boxes\": torch.tensor(boxes)}\n        label_data[image_name] = label\n      index += 1\n    return label_data\n\n\n  def __get_label_file(self) -> List[str]:\n    \"\"\"Returns list containing the lines of the ground truth file.\n    \"\"\"\n    label_file_list = []\n    with open(self.__ground_truth_file) as file:\n      for line in file:\n        label_file_list.append(line.strip())\n    return label_file_list\n\n\n  def __len__(self) -> int:\n    \"\"\"Returns the number of images.\n    \"\"\"\n    return len(self.__paths)\n\n\n  def __get_label(self, index: int) -> List:\n    \"\"\"Returns list with number of faces and bounding boxes for image at position index.\n\n    Attributes:\n      index (int): index of the image to get label data for.\n    \"\"\"\n    image_path = self.__paths[index]\n    path_parts = image_path.split(\"/\")\n    image_name = path_parts[-2] + \"/\" + path_parts[-1]\n    return self.__label_data[image_name]\n\n\n  def __load_image(self, index: int) -> Image.Image:\n    \"\"\"Return Image in Tensor form.\n\n    Attributes:\n      index (int): index of the image to load.\n    \"\"\"\n    image_path = self.__paths[index]\n    image = Image.open(image_path)\n    tensor_transform = ToTensor()\n    tensor_image = tensor_transform(image)\n    return tensor_image\n\n\n  def __getitem__(self, index: int) -> Tuple[torch.Tensor, List]:\n    \"\"\"Returns (transformed) image and label data for given index.\n\n    Attributes:\n      index (int): index for the item to get.\n    \"\"\"\n    image = self.__load_image(index)\n    original_height, original_width = image.shape[1:]\n    label = self.__get_label(index)\n    if not self.__transform:\n      raise ValueError(\"No transform specified!\")\n    image = self.__transform(image)\n    item = {\"image\": image, \"targets\": label}\n    if len(label[\"boxes\"]) == 0:\n      return None\n    else:\n      return item","metadata":{"id":"BJy_qxoemknj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create DataLoaders, Transforms, etc.","metadata":{"id":"Vbak8F3Mrlb9"}},{"cell_type":"code","source":"# auto transformation\nweights = FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT\ntransform = Compose([\n    Resize((320, 320)),\n    weights.transforms()\n    ])","metadata":{"id":"BZp125ldp_aI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_dir = ROOT + \"/widerface/WIDER_train/images/\"\ntrain_gt_file = ROOT + \"/widerface/wider_face_split/wider_face_train_bbx_gt.txt\"\ntrain_dataset = FaceImageDataset(train_data_dir, train_gt_file, transform)","metadata":{"id":"8WkW1oNRrqit","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data_dir = ROOT + \"/widerface/WIDER_val/images/\"\nval_gt_file = ROOT + \"/widerface/wider_face_split/wider_face_val_bbx_gt.txt\"\nval_dataset = FaceImageDataset(val_data_dir, val_gt_file, transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    batch = list(filter(lambda x: x is not None, batch))\n    X = [item[\"image\"] for item in batch]\n    y = [item[\"targets\"] for item in batch]\n    return X, y\ntrain_dataloader = DataLoader(train_dataset, 32, shuffle=True, collate_fn=collate_fn)\nval_dataloader = DataLoader(val_dataset, 32, shuffle=True, collate_fn=collate_fn)","metadata":{"id":"mAnKrj16sSL3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"imJu4ejBsDOg","outputId":"e483cbd2-ef1a-4a0a-9e01-0a0d9257ee76","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create and train model","metadata":{"id":"3f2HEV0uxXOx"}},{"cell_type":"code","source":"model = fasterrcnn_mobilenet_v3_large_320_fpn(weights=weights).to(device)\n# Configure the model for only use case\nnum_classes = 2\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.002, momentum=0.9, weight_decay=0.0010)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kvm8vW2exZ-H","outputId":"d8d04fb8-a8ec-4c7f-af4f-eec483f8d353","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, ROOT + \"/fasterrcnn_complete_model_final.pth\")\nmodel","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ayA1mREVxhNE","outputId":"58ef2fa9-3d0c-4978-d545-b5811ee88d5d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optional if training is splitted into multiple sessions\n#model = torch.load(ROOT + \"/fasterrcnn_complete_model_final.pth\")\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0010)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train()\nmodel.to(device)\nnum_epochs = 30\ntrain_losses = []\nval_losses = []\nfor epoch in range(num_epochs):\n    loss = 0\n    for batch, (X, y) in enumerate(train_dataloader):\n        X = [x.to(device) for x in X]\n        y = [{\"labels\": target[\"labels\"].to(device), \"boxes\": target[\"boxes\"].to(device)} for target in y]\n        # forward pass\n        loss_dict = model(X, y)\n        losses = sum(loss for loss in loss_dict.values())\n        loss += losses\n        # backpropagation\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n    print(f\"Loss after epoch {epoch+1}: {round(float(loss)/len(train_dataloader), 3)}.\")\n    train_losses.append(loss)\n    if epoch % 10 == 0:\n        loss = 0\n        with torch.no_grad():\n            for batch, (X, y) in enumerate(val_dataloader):\n                X = [x.to(device) for x in X]\n                y = [{\"labels\": target[\"labels\"].to(device), \"boxes\": target[\"boxes\"].to(device)} for target in y]\n                # forward pass\n                loss_dict = model(X, y)\n                losses = sum(loss for loss in loss_dict.values())\n                loss += float(losses)\n        print(f\"Loss (val) after epoch {epoch+1}: {round(float(loss)/len(val_dataloader), 3)}.\")\n        val_losses.append(loss)\n        torch.save(model, ROOT + f\"/fasterrcnn_complete_model{epoch//10}.pth\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ibqKhdmx4KY","outputId":"c9e7dab6-6d2e-48e4-88ca-0cd31e35abd0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, ROOT + \"/fasterrcnn_complete_model_final.pth\")","metadata":{"id":"rKfPuUOJ0Ob-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model evaluation (work in progress) test","metadata":{"id":"msikBZmfCp_T"}},{"cell_type":"code","source":"val_data_dir = ROOT + \"/widerface/WIDER_val/images/\"\nval_gt_file = ROOT + \"/widerface/wider_face_split/wider_face_val_bbx_gt.txt\"\nval_dataset = FaceImageDataset(val_data_dir, val_gt_file, transform)","metadata":{"id":"Lpz0gEQkcuqw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_dataloader = DataLoader(val_dataset, 32, shuffle=True, collate_fn=collate_fn)","metadata":{"id":"JBynPH5IMHxD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_loss = 0\nimage_boxes = []\nmodel.eval()\nmodel.to(device)\ncount = 0\nwith torch.no_grad():\n    for batch, (X, y) in enumerate(train_dataloader):\n      if batch < 5:\n        count += len(X)\n        X = [x.to(device) for x in X]\n        y = [{\"labels\": target[\"labels\"].to(device), \"boxes\": target[\"boxes\"].to(device)} for target in y]\n        preds = model(X, y)\n        for x, y, pred in zip(X, y, preds):\n          image_boxes.append((x, pred[\"boxes\"], y[\"boxes\"], pred[\"scores\"]))\n      else:\n        break\nimage_boxes[0]","metadata":{"id":"UD08efM_sZgP","outputId":"d939eadf-ccb6-4be2-eb6e-80a5af2763b4","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_image_pred_boxes(axs, x, y, image, boxes_pred, boxes_truth):\n  axs[x, y].imshow(image)\n  for rect in boxes_pred:\n    x_0, y_0, x_1, y_1 = rect\n    rect_obj = patches.Rectangle((min(x_0, x_1), min(y_0, y_1)), abs(x_0-x_1), abs(y_0-y_1), linewidth=2, edgecolor='r', facecolor='none', alpha=0.5)\n    axs[x, y].add_patch(rect_obj)\n  for rect in boxes_truth:\n    x_0, y_0, x_1, y_1 = rect\n    rect_obj = patches.Rectangle((min(x_0, x_1), min(y_0, y_1)), abs(x_0-x_1), abs(y_0-y_1), linewidth=2, edgecolor='green', facecolor='none')\n    axs[x, y].add_patch(rect_obj)","metadata":{"id":"LlkvwMox4VhF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n\ndata = random.sample(image_boxes, 4)\nplot_image_pred_boxes(axs, 0, 0, data[0][0].cpu().numpy().transpose(1, 2, 0), data[0][1].cpu().numpy(), data[0][2].cpu().numpy())\nplot_image_pred_boxes(axs, 1, 0, data[1][0].cpu().numpy().transpose(1, 2, 0), data[1][1].cpu().numpy(), data[1][2].cpu().numpy())\nplot_image_pred_boxes(axs, 0, 1, data[2][0].cpu().numpy().transpose(1, 2, 0), data[2][1].cpu().numpy(), data[2][2].cpu().numpy())\nplot_image_pred_boxes(axs, 1, 1, data[3][0].cpu().numpy().transpose(1, 2, 0), data[3][1].cpu().numpy(), data[3][2].cpu().numpy())","metadata":{"id":"v8otZPva8erI","outputId":"f19e95dd-929f-41e1-ace6-51c2509ab0bd","colab":{"base_uri":"https://localhost:8080/","height":716},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_boxes[8]","metadata":{"id":"mlaZzrig86h_","outputId":"ea94eee4-b8eb-4f07-c1a8-1d1fae303552","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"1TnBeQTPBtBv"},"execution_count":null,"outputs":[]}]}